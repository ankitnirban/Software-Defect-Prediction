{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8130da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 err: 1388119.5\n",
      "Step 1 err: 1373705.5\n",
      "Step 2 err: 1291398.5\n",
      "Step 3 err: 1251436.5\n",
      "Step 4 err: 1183684.5\n",
      "Step 5 err: 1128337.5\n",
      "Step 6 err: 1054474.5\n",
      "Step 7 err: 989355.5\n",
      "Step 8 err: 942212.5\n",
      "Step 9 err: 902949.5\n",
      "Step 10 err: 899901.5\n",
      "Step 11 err: 845313.5\n",
      "Step 12 err: 836871.5\n",
      "Step 13 err: 802154.5\n",
      "Step 14 err: 797646.5\n",
      "Step 15 err: 796987.5\n",
      "Step 16 err: 780758.5\n",
      "Step 17 err: 760586.5\n",
      "Step 18 err: 756303.5\n",
      "Step 19 err: 697902.5\n",
      "Step 20 err: 634035.5\n",
      "Step 21 err: 619607.5\n",
      "Step 22 err: 608578.5\n",
      "Step 23 err: 607550.5\n",
      "Step 24 err: 583414.5\n",
      "Step 25 err: 571303.5\n",
      "Step 26 err: 559575.5\n",
      "Step 27 err: 559557.5\n",
      "Step 28 err: 558327.5\n",
      "Step 29 err: 549722.5\n",
      "Step 30 err: 549722.5\n",
      "Step 31 err: 539029.5\n",
      "Step 32 err: 539029.5\n",
      "Step 33 err: 539029.5\n",
      "Step 34 err: 539029.5\n",
      "Step 35 err: 539029.5\n",
      "Step 36 err: 539029.5\n",
      "Step 37 err: 537474.5\n",
      "Step 38 err: 535447.5\n",
      "Step 39 err: 535447.5\n",
      "Step 40 err: 534075.5\n",
      "Step 41 err: 534075.5\n",
      "Step 42 err: 534075.5\n",
      "Step 43 err: 534075.5\n",
      "Step 44 err: 534075.5\n",
      "Step 45 err: 533317.5\n",
      "Step 46 err: 533317.5\n",
      "Step 47 err: 533317.5\n",
      "Step 48 err: 533317.5\n",
      "Step 49 err: 533317.5\n",
      "Step 50 err: 533317.5\n",
      "Step 51 err: 533317.5\n",
      "Step 52 err: 533317.5\n",
      "Step 53 err: 533317.5\n",
      "Step 54 err: 533317.5\n",
      "Step 55 err: 533317.5\n",
      "Step 56 err: 533317.5\n",
      "Step 57 err: 532762.5\n",
      "Step 58 err: 532762.5\n",
      "Step 59 err: 532762.5\n",
      "Step 60 err: 532762.5\n",
      "Step 61 err: 532762.5\n",
      "Step 62 err: 532762.5\n",
      "Step 63 err: 532762.5\n",
      "Step 64 err: 532762.5\n",
      "Step 65 err: 532762.5\n",
      "Step 66 err: 532762.5\n",
      "Step 67 err: 532762.5\n",
      "Step 68 err: 532762.5\n",
      "Step 69 err: 532762.5\n",
      "Step 70 err: 532762.5\n",
      "Step 71 err: 532762.5\n",
      "Step 72 err: 532762.5\n",
      "Step 73 err: 532762.5\n",
      "Step 74 err: 532762.5\n",
      "Step 75 err: 532762.5\n",
      "Step 76 err: 532762.5\n",
      "Step 77 err: 532762.5\n",
      "Step 78 err: 532762.5\n",
      "Step 79 err: 532762.5\n",
      "Step 80 err: 532762.5\n",
      "Step 81 err: 532762.5\n",
      "Step 82 err: 532762.5\n",
      "Step 83 err: 532762.5\n",
      "Step 84 err: 532762.5\n",
      "Step 85 err: 532762.5\n",
      "Step 86 err: 532762.5\n",
      "Step 87 err: 532762.5\n",
      "Step 88 err: 532762.5\n",
      "Step 89 err: 532762.5\n",
      "Step 90 err: 532762.5\n",
      "Step 91 err: 532762.5\n",
      "Step 92 err: 532762.5\n",
      "Step 93 err: 532762.5\n",
      "Step 94 err: 532762.5\n",
      "Step 95 err: 532762.5\n",
      "Step 96 err: 532762.5\n",
      "Step 97 err: 532762.5\n",
      "Step 98 err: 532762.5\n",
      "Step 99 err: 532762.5\n",
      "Step 100 err: 532762.5\n",
      "Step 101 err: 532762.5\n",
      "Step 102 err: 532762.5\n",
      "Step 103 err: 532762.5\n",
      "Step 104 err: 532762.5\n",
      "Step 105 err: 532762.5\n",
      "Step 106 err: 532762.5\n",
      "Step 107 err: 532762.5\n",
      "Step 108 err: 532762.5\n",
      "Step 109 err: 532762.5\n",
      "Step 110 err: 532762.5\n",
      "Step 111 err: 532762.5\n",
      "Step 112 err: 532762.5\n",
      "Step 113 err: 532762.5\n",
      "Step 114 err: 532762.5\n",
      "Step 115 err: 532762.5\n",
      "Step 116 err: 532762.5\n",
      "Step 117 err: 532762.5\n",
      "Step 118 err: 532762.5\n",
      "Step 119 err: 532762.5\n",
      "Step 120 err: 532762.5\n",
      "Step 121 err: 532762.5\n",
      "Step 122 err: 532762.5\n",
      "Step 123 err: 532762.5\n",
      "Step 124 err: 532762.5\n",
      "Step 125 err: 532762.5\n",
      "Step 126 err: 532762.5\n",
      "Step 127 err: 532762.5\n",
      "Step 128 err: 532762.5\n",
      "Step 129 err: 532762.5\n",
      "Step 130 err: 532762.5\n",
      "Step 131 err: 532762.5\n",
      "Step 132 err: 532762.5\n",
      "Step 133 err: 532762.5\n",
      "Step 134 err: 532762.5\n",
      "Step 135 err: 532762.5\n",
      "Step 136 err: 532762.5\n",
      "Step 137 err: 532762.5\n",
      "Step 138 err: 532762.5\n",
      "Step 139 err: 532762.5\n",
      "Step 140 err: 532762.5\n",
      "Step 141 err: 532762.5\n",
      "Step 142 err: 532762.5\n",
      "Step 143 err: 532762.5\n",
      "Step 144 err: 532762.5\n",
      "Step 145 err: 532762.5\n",
      "Step 146 err: 532762.5\n",
      "Step 147 err: 532762.5\n",
      "Step 148 err: 532762.5\n",
      "Step 149 err: 532762.5\n",
      "Step 150 err: 532762.5\n",
      "Step 151 err: 532762.5\n",
      "Step 152 err: 532762.5\n",
      "Step 153 err: 532762.5\n",
      "Step 154 err: 532762.5\n",
      "Step 155 err: 532762.5\n",
      "Step 156 err: 532762.5\n",
      "Step 157 err: 532762.5\n",
      "Step 158 err: 532762.5\n",
      "Step 159 err: 532762.5\n",
      "Step 160 err: 532762.5\n",
      "Step 161 err: 532762.5\n",
      "Step 162 err: 532762.5\n",
      "Step 163 err: 532762.5\n",
      "Step 164 err: 532762.5\n",
      "Step 165 err: 532762.5\n",
      "Step 166 err: 532762.5\n",
      "Step 167 err: 532762.5\n",
      "Step 168 err: 532762.5\n",
      "Step 169 err: 532762.5\n",
      "Step 170 err: 532762.5\n",
      "Step 171 err: 532762.5\n",
      "Step 172 err: 532762.5\n",
      "Step 173 err: 532762.5\n",
      "Step 174 err: 532762.5\n",
      "Step 175 err: 532762.5\n",
      "Step 176 err: 532762.5\n",
      "Step 177 err: 532762.5\n",
      "Step 178 err: 532762.5\n",
      "Step 179 err: 532762.5\n",
      "Step 180 err: 532762.5\n",
      "Step 181 err: 532762.5\n",
      "Step 182 err: 532762.5\n",
      "Step 183 err: 532762.5\n",
      "Step 184 err: 532762.5\n",
      "Step 185 err: 532762.5\n",
      "Step 186 err: 532762.5\n",
      "Step 187 err: 532762.5\n",
      "Step 188 err: 532762.5\n",
      "Step 189 err: 532762.5\n",
      "Step 190 err: 532762.5\n",
      "Step 191 err: 532762.5\n",
      "Step 192 err: 532762.5\n",
      "Step 193 err: 532762.5\n",
      "Step 194 err: 532762.5\n",
      "Step 195 err: 532762.5\n",
      "Step 196 err: 532762.5\n",
      "Step 197 err: 532762.5\n",
      "Step 198 err: 532762.5\n",
      "Step 199 err: 532762.5\n",
      "Step 200 err: 532762.5\n",
      "Step 201 err: 532762.5\n",
      "Step 202 err: 532762.5\n",
      "Step 203 err: 532762.5\n",
      "Step 204 err: 532762.5\n",
      "Step 205 err: 532762.5\n",
      "Step 206 err: 532762.5\n",
      "Step 207 err: 532762.5\n",
      "Step 208 err: 532762.5\n",
      "Step 209 err: 532762.5\n",
      "Step 210 err: 532762.5\n",
      "Step 211 err: 532762.5\n",
      "Step 212 err: 532762.5\n",
      "Step 213 err: 532762.5\n",
      "Step 214 err: 532762.5\n",
      "Step 215 err: 532762.5\n",
      "Step 216 err: 532762.5\n",
      "Step 217 err: 532762.5\n",
      "Step 218 err: 532762.5\n",
      "Step 219 err: 532762.5\n",
      "Step 220 err: 532762.5\n",
      "Step 221 err: 532762.5\n",
      "Step 222 err: 532762.5\n",
      "Step 223 err: 532762.5\n",
      "Step 224 err: 532762.5\n",
      "Step 225 err: 532762.5\n",
      "Step 226 err: 532762.5\n",
      "Step 227 err: 532762.5\n",
      "Step 228 err: 532762.5\n",
      "Step 229 err: 532762.5\n",
      "Step 230 err: 532762.5\n",
      "Step 231 err: 532762.5\n",
      "Step 232 err: 532762.5\n",
      "Step 233 err: 532762.5\n",
      "Step 234 err: 532762.5\n",
      "Step 235 err: 532762.5\n",
      "Step 236 err: 532762.5\n",
      "Step 237 err: 532762.5\n",
      "Step 238 err: 532762.5\n",
      "Step 239 err: 532762.5\n",
      "Step 240 err: 532762.5\n",
      "Step 241 err: 532762.5\n",
      "Step 242 err: 532762.5\n",
      "Step 243 err: 532762.5\n",
      "Step 244 err: 532762.5\n",
      "Step 245 err: 532762.5\n",
      "Step 246 err: 532762.5\n",
      "Step 247 err: 532762.5\n",
      "Step 248 err: 532762.5\n",
      "Step 249 err: 532762.5\n",
      "Step 250 err: 532762.5\n",
      "Step 251 err: 532762.5\n",
      "Step 252 err: 532762.5\n",
      "Step 253 err: 532762.5\n",
      "Step 254 err: 532762.5\n",
      "Step 255 err: 532762.5\n",
      "Step 256 err: 532762.5\n",
      "Step 257 err: 532762.5\n",
      "Step 258 err: 532762.5\n",
      "Step 259 err: 532762.5\n",
      "Step 260 err: 532762.5\n",
      "Step 261 err: 532762.5\n",
      "Step 262 err: 532762.5\n",
      "Step 263 err: 532762.5\n",
      "Step 264 err: 532762.5\n",
      "Step 265 err: 532762.5\n",
      "Step 266 err: 532762.5\n",
      "Step 267 err: 532762.5\n",
      "Step 268 err: 532762.5\n",
      "Step 269 err: 532762.5\n",
      "Step 270 err: 532762.5\n",
      "Step 271 err: 532762.5\n",
      "Step 272 err: 532762.5\n",
      "Step 273 err: 532762.5\n",
      "Step 274 err: 532762.5\n",
      "Step 275 err: 532762.5\n",
      "Step 276 err: 532762.5\n",
      "Step 277 err: 532762.5\n",
      "Step 278 err: 532762.5\n",
      "Step 279 err: 532762.5\n",
      "Step 280 err: 532762.5\n",
      "Step 281 err: 532762.5\n",
      "Step 282 err: 532762.5\n",
      "Step 283 err: 532762.5\n",
      "Step 284 err: 532762.5\n",
      "Step 285 err: 532762.5\n",
      "Step 286 err: 532762.5\n",
      "Step 287 err: 532762.5\n",
      "Step 288 err: 532762.5\n",
      "Step 289 err: 532762.5\n",
      "Step 290 err: 532762.5\n",
      "Step 291 err: 532762.5\n",
      "Step 292 err: 532762.5\n",
      "Step 293 err: 532762.5\n",
      "Step 294 err: 532762.5\n",
      "Step 295 err: 532762.5\n",
      "Step 296 err: 532762.5\n",
      "Step 297 err: 532762.5\n",
      "Step 298 err: 532762.5\n",
      "Step 299 err: 532762.5\n",
      "Step 300 err: 532762.5\n",
      "Step 301 err: 532762.5\n",
      "Step 302 err: 532762.5\n",
      "Step 303 err: 532762.5\n",
      "Step 304 err: 532762.5\n",
      "Step 305 err: 532762.5\n",
      "Step 306 err: 532762.5\n",
      "Step 307 err: 532762.5\n",
      "Step 308 err: 532762.5\n",
      "Step 309 err: 532762.5\n",
      "Step 310 err: 532762.5\n",
      "Step 311 err: 532762.5\n",
      "Step 312 err: 532762.5\n",
      "Step 313 err: 532762.5\n",
      "Step 314 err: 532762.5\n",
      "Step 315 err: 532762.5\n",
      "Step 316 err: 532762.5\n",
      "Step 317 err: 532762.5\n",
      "Step 318 err: 532762.5\n",
      "Step 319 err: 532762.5\n",
      "Step 320 err: 532762.5\n",
      "Step 321 err: 532762.5\n",
      "Step 322 err: 532762.5\n",
      "Step 323 err: 532762.5\n",
      "Step 324 err: 532762.5\n",
      "Step 325 err: 532762.5\n",
      "Step 326 err: 532762.5\n",
      "Step 327 err: 532762.5\n",
      "Step 328 err: 532762.5\n",
      "Step 329 err: 532762.5\n",
      "Step 330 err: 532762.5\n",
      "Step 331 err: 532762.5\n",
      "Step 332 err: 532762.5\n",
      "Step 333 err: 532762.5\n",
      "Step 334 err: 532762.5\n",
      "Step 335 err: 532762.5\n",
      "Step 336 err: 532762.5\n",
      "Step 337 err: 532762.5\n",
      "Step 338 err: 532762.5\n",
      "Step 339 err: 532762.5\n",
      "Step 340 err: 532762.5\n",
      "Step 341 err: 532762.5\n",
      "Step 342 err: 532762.5\n",
      "Step 343 err: 532762.5\n",
      "Step 344 err: 532762.5\n",
      "Step 345 err: 532762.5\n",
      "Step 346 err: 532762.5\n",
      "Step 347 err: 532762.5\n",
      "Step 348 err: 532762.5\n",
      "Step 349 err: 532762.5\n",
      "Step 350 err: 532762.5\n",
      "Step 351 err: 532762.5\n",
      "Step 352 err: 532762.5\n",
      "Step 353 err: 532762.5\n",
      "Step 354 err: 532762.5\n",
      "Step 355 err: 532762.5\n",
      "Step 356 err: 532762.5\n",
      "Step 357 err: 532762.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Preprocess import pre_process\n",
    "ds=pd.read_csv(r\"C:\\Users\\ankit\\Desktop\\Alexnet project\\NASA_dataset\\CM1.csv\")\n",
    "data_set=pre_process(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdde943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf121e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b985ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "image_list=[]\n",
    "for filename in glob.glob('C:/Users/ankit/Desktop/Alexnet project/Results/Test_1/data/*.png'):\n",
    "    im=Image.open(filename)\n",
    "    im=im.convert('RGB')\n",
    "    image_list.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6984533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 366, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "img1=asarray(image_list[0])\n",
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e59fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>v(g)</th>\n",
       "      <th>ev(g)</th>\n",
       "      <th>iv(g)</th>\n",
       "      <th>n</th>\n",
       "      <th>v</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>i</th>\n",
       "      <th>e</th>\n",
       "      <th>...</th>\n",
       "      <th>lOCode</th>\n",
       "      <th>lOComment</th>\n",
       "      <th>lOBlank</th>\n",
       "      <th>locCodeAndComment</th>\n",
       "      <th>uniq_Op</th>\n",
       "      <th>uniq_Opnd</th>\n",
       "      <th>total_Op</th>\n",
       "      <th>total_Opnd</th>\n",
       "      <th>branchCount</th>\n",
       "      <th>defects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>6.036150e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>4.643192e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054502</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.029894</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.110801</td>\n",
       "      <td>1.363599e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.047771</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.023342</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045024</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.022179</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.127216</td>\n",
       "      <td>0.045866</td>\n",
       "      <td>1.600922e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.037267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054502</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.034233</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.137791</td>\n",
       "      <td>0.067999</td>\n",
       "      <td>2.785720e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211268</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.031941</td>\n",
       "      <td>0.062112</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0.064355</td>\n",
       "      <td>0.024372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.068065</td>\n",
       "      <td>0.046996</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.140497</td>\n",
       "      <td>0.156285</td>\n",
       "      <td>6.677613e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.037565</td>\n",
       "      <td>0.231403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272047</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.069970</td>\n",
       "      <td>0.065116</td>\n",
       "      <td>0.028762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0.050689</td>\n",
       "      <td>0.012023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>0.055478</td>\n",
       "      <td>0.034442</td>\n",
       "      <td>0.045880</td>\n",
       "      <td>0.123540</td>\n",
       "      <td>0.128886</td>\n",
       "      <td>4.274671e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015165</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.165501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158433</td>\n",
       "      <td>0.068463</td>\n",
       "      <td>0.048066</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.067258</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041180</td>\n",
       "      <td>0.074078</td>\n",
       "      <td>0.051274</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.147984</td>\n",
       "      <td>0.160740</td>\n",
       "      <td>7.583576e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042287</td>\n",
       "      <td>0.047951</td>\n",
       "      <td>0.228724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263710</td>\n",
       "      <td>0.099946</td>\n",
       "      <td>0.074856</td>\n",
       "      <td>0.072874</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.147694</td>\n",
       "      <td>0.094270</td>\n",
       "      <td>0.068201</td>\n",
       "      <td>0.075150</td>\n",
       "      <td>0.091668</td>\n",
       "      <td>0.066965</td>\n",
       "      <td>0.048604</td>\n",
       "      <td>0.121981</td>\n",
       "      <td>0.262958</td>\n",
       "      <td>8.051069e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145187</td>\n",
       "      <td>0.181845</td>\n",
       "      <td>0.072445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234950</td>\n",
       "      <td>0.147158</td>\n",
       "      <td>0.088548</td>\n",
       "      <td>0.096497</td>\n",
       "      <td>0.111251</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.052440</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019773</td>\n",
       "      <td>0.054647</td>\n",
       "      <td>0.033575</td>\n",
       "      <td>0.044416</td>\n",
       "      <td>0.129259</td>\n",
       "      <td>0.122279</td>\n",
       "      <td>4.339318e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.051049</td>\n",
       "      <td>0.157746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158112</td>\n",
       "      <td>0.065080</td>\n",
       "      <td>0.047274</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>0.013826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loc      v(g)     ev(g)     iv(g)        n          v        l   \\\n",
       "0    0.000237  0.004211  0.013793  0.006452  0.000145  0.000076  1.000000   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000058  0.769231   \n",
       "2    0.054502  0.042105  0.000000  0.032258  0.029894  0.018052  0.084615   \n",
       "3    0.045024  0.031579  0.103448  0.016129  0.022179  0.012584  0.046154   \n",
       "4    0.054502  0.052632  0.172414  0.016129  0.034233  0.020213  0.046154   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "893  0.064355  0.024372  0.000000  0.032258  0.068065  0.046996  0.042100   \n",
       "894  0.050689  0.012023  0.000000  0.017849  0.055478  0.034442  0.045880   \n",
       "895  0.067258  0.029787  0.000000  0.041180  0.074078  0.051274  0.038462   \n",
       "896  0.147694  0.094270  0.068201  0.075150  0.091668  0.066965  0.048604   \n",
       "897  0.052440  0.012905  0.000000  0.019773  0.054647  0.033575  0.044416   \n",
       "\n",
       "            d         i             e  ...    lOCode   lOComment   lOBlank  \\\n",
       "0    0.010336  0.004427  6.036150e-07  ...  0.025000    0.005900  0.012195   \n",
       "1    0.007951  0.003405  4.643192e-07  ...  0.012500    0.002950  0.006098   \n",
       "2    0.075535  0.110801  1.363599e-03  ...  0.012500    0.000000  0.036585   \n",
       "3    0.127216  0.045866  1.600922e-03  ...  0.000000    0.000000  0.018293   \n",
       "4    0.137791  0.067999  2.785720e-03  ...  0.000000    0.000000  0.018293   \n",
       "..        ...       ...           ...  ...       ...         ...       ...   \n",
       "893  0.140497  0.156285  6.677613e-03  ...  0.028942    0.037565  0.231403   \n",
       "894  0.123540  0.128886  4.274671e-03  ...  0.015165    0.057200  0.165501   \n",
       "895  0.147984  0.160740  7.583576e-03  ...  0.042287    0.047951  0.228724   \n",
       "896  0.121981  0.262958  8.051069e-03  ...  0.145187    0.181845  0.072445   \n",
       "897  0.129259  0.122279  4.339318e-03  ...  0.011088    0.051049  0.157746   \n",
       "\n",
       "      locCodeAndComment   uniq_Op   uniq_Opnd   total_Op   total_Opnd  \\\n",
       "0                   1.0  0.002817    0.003822   0.000159     0.001474   \n",
       "1                   0.5  0.000000    0.003185   0.000000     0.001229   \n",
       "2                   0.0  0.197183    0.047771   0.034127     0.023342   \n",
       "3                   0.0  0.211268    0.025478   0.023810     0.019656   \n",
       "4                   0.0  0.211268    0.038217   0.035714     0.031941   \n",
       "..                  ...       ...         ...        ...          ...   \n",
       "893                 0.0  0.272047    0.097048   0.069970     0.065116   \n",
       "894                 0.0  0.158433    0.068463   0.048066     0.066950   \n",
       "895                 0.0  0.263710    0.099946   0.074856     0.072874   \n",
       "896                 0.0  0.234950    0.147158   0.088548     0.096497   \n",
       "897                 0.0  0.158112    0.065080   0.047274     0.066061   \n",
       "\n",
       "      branchCount   defects  \n",
       "0        0.002484       0.0  \n",
       "1        0.000000       1.0  \n",
       "2        0.049689       0.0  \n",
       "3        0.037267       0.0  \n",
       "4        0.062112       0.0  \n",
       "..            ...       ...  \n",
       "893      0.028762       1.0  \n",
       "894      0.014188       1.0  \n",
       "895      0.035153       1.0  \n",
       "896      0.111251       1.0  \n",
       "897      0.013826       1.0  \n",
       "\n",
       "[898 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6931870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll=list(data_set[data_set.columns[-1]])\n",
    "label_list=[]\n",
    "for each in ll:\n",
    "    if each==True:\n",
    "        label_list.append(1)\n",
    "    else:\n",
    "        label_list.append(0)\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1be6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp/ipykernel_12668/2225610901.py:2: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  image_list=np.array(image_list)\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp/ipykernel_12668/2225610901.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  image_list=np.array(image_list)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "image_list=np.array(image_list)\n",
    "label_list=np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b5089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 898)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_list), len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8cf1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 43, 810)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, test_images, train_labels, test_labels= train_test_split(image_list, label_list, test_size=0.05, shuffle=True, random_state=8)\n",
    "train_images, val_images, train_labels, val_labels=train_test_split(train_images, train_labels, test_size=0.05, shuffle=True, random_state=8)\n",
    "len(test_labels), len(val_labels), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5519da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "img=train_images[0]\n",
    "img=asarray(img)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((np.array([asarray(image) for image in train_images]), train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((np.array([asarray(image) for image in test_images]), test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((np.array([asarray(image) for image in val_images]), val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c64ce399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    # Resize images to 277x277\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b1cd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 227, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label=process_images(img, train_labels[0])\n",
    "img=asarray(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf56c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "45\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "train_ds_size=tf.data.experimental.cardinality(train_ds).numpy()\n",
    "test_ds_size=tf.data.experimental.cardinality(test_ds).numpy()\n",
    "validation_ds_size=tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "print(train_ds_size)\n",
    "print(test_ds_size)\n",
    "print(validation_ds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050a59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=(train_ds.map(process_images).shuffle(buffer_size=train_ds_size).batch(batch_size=4, drop_remainder=True))\n",
    "test_ds=(test_ds.map(process_images).shuffle(buffer_size=train_ds_size).batch(batch_size=4, drop_remainder=True))\n",
    "validation_ds=(validation_ds.map(process_images).shuffle(buffer_size=train_ds_size).batch(batch_size=4,drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3e438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd6a74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet model implementation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca30f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,290,945\n",
      "Trainable params: 58,288,193\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.SGD(learning_rate=0.001), metrics=['accuracy'], run_eagerly=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef6409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "202/202 [==============================] - 85s 420ms/step - loss: 1.8953 - accuracy: 0.5743 - val_loss: 0.5538 - val_accuracy: 0.8000\n",
      "Epoch 2/150\n",
      "202/202 [==============================] - 86s 426ms/step - loss: 1.0982 - accuracy: 0.6312 - val_loss: 0.4056 - val_accuracy: 0.8500\n",
      "Epoch 3/150\n",
      "202/202 [==============================] - 86s 426ms/step - loss: 0.8589 - accuracy: 0.6485 - val_loss: 0.5184 - val_accuracy: 0.8250\n",
      "Epoch 4/150\n",
      "202/202 [==============================] - 84s 414ms/step - loss: 0.8125 - accuracy: 0.6609 - val_loss: 0.5367 - val_accuracy: 0.8250\n",
      "Epoch 5/150\n",
      "202/202 [==============================] - 84s 416ms/step - loss: 0.7147 - accuracy: 0.6733 - val_loss: 0.5891 - val_accuracy: 0.7500\n",
      "Epoch 6/150\n",
      "202/202 [==============================] - 84s 416ms/step - loss: 0.7280 - accuracy: 0.7054 - val_loss: 0.5459 - val_accuracy: 0.7250\n",
      "Epoch 7/150\n",
      "202/202 [==============================] - 84s 417ms/step - loss: 0.6259 - accuracy: 0.6881 - val_loss: 0.4080 - val_accuracy: 0.8000\n",
      "Epoch 8/150\n",
      "202/202 [==============================] - 84s 414ms/step - loss: 0.6055 - accuracy: 0.7166 - val_loss: 0.8802 - val_accuracy: 0.5750\n",
      "Epoch 9/150\n",
      "202/202 [==============================] - 84s 417ms/step - loss: 0.6178 - accuracy: 0.7017 - val_loss: 0.4854 - val_accuracy: 0.8000\n",
      "Epoch 10/150\n",
      "202/202 [==============================] - 85s 422ms/step - loss: 0.5822 - accuracy: 0.7191 - val_loss: 0.5632 - val_accuracy: 0.8250\n",
      "Epoch 11/150\n",
      "202/202 [==============================] - 90s 444ms/step - loss: 0.5581 - accuracy: 0.7290 - val_loss: 0.4702 - val_accuracy: 0.8500\n",
      "Epoch 12/150\n",
      "202/202 [==============================] - 90s 446ms/step - loss: 0.5336 - accuracy: 0.7426 - val_loss: 0.5389 - val_accuracy: 0.8000\n",
      "Epoch 13/150\n",
      "202/202 [==============================] - 88s 434ms/step - loss: 0.5064 - accuracy: 0.7587 - val_loss: 0.3499 - val_accuracy: 0.8750\n",
      "Epoch 14/150\n",
      "202/202 [==============================] - 86s 424ms/step - loss: 0.5203 - accuracy: 0.7537 - val_loss: 0.3433 - val_accuracy: 0.8250\n",
      "Epoch 15/150\n",
      "202/202 [==============================] - 86s 424ms/step - loss: 0.5260 - accuracy: 0.7488 - val_loss: 0.4751 - val_accuracy: 0.8500\n",
      "Epoch 16/150\n",
      "202/202 [==============================] - 85s 420ms/step - loss: 0.4874 - accuracy: 0.7748 - val_loss: 0.3733 - val_accuracy: 0.8750\n",
      "Epoch 17/150\n",
      "202/202 [==============================] - 86s 426ms/step - loss: 0.4999 - accuracy: 0.7611 - val_loss: 0.5869 - val_accuracy: 0.7500\n",
      "Epoch 18/150\n",
      "202/202 [==============================] - 86s 424ms/step - loss: 0.4769 - accuracy: 0.7884 - val_loss: 0.3930 - val_accuracy: 0.8500\n",
      "Epoch 19/150\n",
      "202/202 [==============================] - 82s 407ms/step - loss: 0.4788 - accuracy: 0.7710 - val_loss: 0.5991 - val_accuracy: 0.7250\n",
      "Epoch 20/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.4547 - accuracy: 0.7723 - val_loss: 0.4721 - val_accuracy: 0.7750\n",
      "Epoch 21/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.4793 - accuracy: 0.7760 - val_loss: 0.5306 - val_accuracy: 0.8250\n",
      "Epoch 22/150\n",
      "202/202 [==============================] - 84s 414ms/step - loss: 0.4594 - accuracy: 0.7871 - val_loss: 0.5970 - val_accuracy: 0.8250\n",
      "Epoch 23/150\n",
      "202/202 [==============================] - 84s 414ms/step - loss: 0.4437 - accuracy: 0.7933 - val_loss: 0.5736 - val_accuracy: 0.8500\n",
      "Epoch 24/150\n",
      "202/202 [==============================] - 83s 410ms/step - loss: 0.4291 - accuracy: 0.7958 - val_loss: 0.4579 - val_accuracy: 0.8250\n",
      "Epoch 25/150\n",
      "202/202 [==============================] - 83s 409ms/step - loss: 0.4430 - accuracy: 0.8045 - val_loss: 0.4501 - val_accuracy: 0.8250\n",
      "Epoch 26/150\n",
      "202/202 [==============================] - 88s 436ms/step - loss: 0.4359 - accuracy: 0.8045 - val_loss: 0.4469 - val_accuracy: 0.8000\n",
      "Epoch 27/150\n",
      "202/202 [==============================] - 84s 416ms/step - loss: 0.4252 - accuracy: 0.7995 - val_loss: 0.5287 - val_accuracy: 0.7500\n",
      "Epoch 28/150\n",
      "202/202 [==============================] - 83s 411ms/step - loss: 0.4131 - accuracy: 0.7995 - val_loss: 0.4568 - val_accuracy: 0.8250\n",
      "Epoch 29/150\n",
      "202/202 [==============================] - 83s 411ms/step - loss: 0.4111 - accuracy: 0.8181 - val_loss: 0.5257 - val_accuracy: 0.8750\n",
      "Epoch 30/150\n",
      "202/202 [==============================] - 83s 410ms/step - loss: 0.4040 - accuracy: 0.8119 - val_loss: 0.5518 - val_accuracy: 0.7750\n",
      "Epoch 31/150\n",
      "202/202 [==============================] - 83s 410ms/step - loss: 0.3888 - accuracy: 0.8366 - val_loss: 0.3536 - val_accuracy: 0.8500\n",
      "Epoch 32/150\n",
      "202/202 [==============================] - 83s 412ms/step - loss: 0.4123 - accuracy: 0.8168 - val_loss: 0.4717 - val_accuracy: 0.8500\n",
      "Epoch 33/150\n",
      "202/202 [==============================] - 86s 424ms/step - loss: 0.4069 - accuracy: 0.8342 - val_loss: 0.4976 - val_accuracy: 0.8500\n",
      "Epoch 34/150\n",
      "202/202 [==============================] - 85s 420ms/step - loss: 0.4084 - accuracy: 0.8255 - val_loss: 0.4380 - val_accuracy: 0.8500\n",
      "Epoch 35/150\n",
      "202/202 [==============================] - 90s 444ms/step - loss: 0.3723 - accuracy: 0.8391 - val_loss: 0.4014 - val_accuracy: 0.8500\n",
      "Epoch 36/150\n",
      "202/202 [==============================] - 85s 418ms/step - loss: 0.3754 - accuracy: 0.8243 - val_loss: 0.3890 - val_accuracy: 0.9000\n",
      "Epoch 37/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.3826 - accuracy: 0.8329 - val_loss: 0.4322 - val_accuracy: 0.8500\n",
      "Epoch 38/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.3748 - accuracy: 0.8329 - val_loss: 0.3945 - val_accuracy: 0.8500\n",
      "Epoch 39/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.3823 - accuracy: 0.8119 - val_loss: 0.4903 - val_accuracy: 0.8000\n",
      "Epoch 40/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.3647 - accuracy: 0.8465 - val_loss: 0.4495 - val_accuracy: 0.8500\n",
      "Epoch 41/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.3793 - accuracy: 0.8465 - val_loss: 0.5390 - val_accuracy: 0.7750\n",
      "Epoch 42/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.3460 - accuracy: 0.8465 - val_loss: 0.4836 - val_accuracy: 0.8000\n",
      "Epoch 43/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.3527 - accuracy: 0.8478 - val_loss: 0.4768 - val_accuracy: 0.8500\n",
      "Epoch 44/150\n",
      "202/202 [==============================] - 83s 408ms/step - loss: 0.3520 - accuracy: 0.8379 - val_loss: 0.5428 - val_accuracy: 0.8000\n",
      "Epoch 45/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3330 - accuracy: 0.8663 - val_loss: 0.5552 - val_accuracy: 0.7750\n",
      "Epoch 46/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3341 - accuracy: 0.8614 - val_loss: 0.5128 - val_accuracy: 0.8000\n",
      "Epoch 47/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3494 - accuracy: 0.8490 - val_loss: 0.4708 - val_accuracy: 0.8000\n",
      "Epoch 48/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3308 - accuracy: 0.8478 - val_loss: 0.4794 - val_accuracy: 0.8000\n",
      "Epoch 49/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3517 - accuracy: 0.8527 - val_loss: 0.5463 - val_accuracy: 0.7750\n",
      "Epoch 50/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.3674 - accuracy: 0.8465 - val_loss: 0.5090 - val_accuracy: 0.8250\n",
      "Epoch 51/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3235 - accuracy: 0.8614 - val_loss: 0.3402 - val_accuracy: 0.8500\n",
      "Epoch 52/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3243 - accuracy: 0.8540 - val_loss: 0.4026 - val_accuracy: 0.8500\n",
      "Epoch 53/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3178 - accuracy: 0.8639 - val_loss: 0.5679 - val_accuracy: 0.7750\n",
      "Epoch 54/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3180 - accuracy: 0.8750 - val_loss: 0.4891 - val_accuracy: 0.7750\n",
      "Epoch 55/150\n",
      "202/202 [==============================] - 81s 400ms/step - loss: 0.3562 - accuracy: 0.8540 - val_loss: 0.4661 - val_accuracy: 0.8250\n",
      "Epoch 56/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3244 - accuracy: 0.8639 - val_loss: 0.3397 - val_accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3201 - accuracy: 0.8762 - val_loss: 0.4545 - val_accuracy: 0.8000\n",
      "Epoch 58/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3013 - accuracy: 0.8713 - val_loss: 0.5243 - val_accuracy: 0.7750\n",
      "Epoch 59/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3023 - accuracy: 0.8700 - val_loss: 0.3718 - val_accuracy: 0.8500\n",
      "Epoch 60/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3209 - accuracy: 0.8614 - val_loss: 0.5977 - val_accuracy: 0.8000\n",
      "Epoch 61/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3151 - accuracy: 0.8663 - val_loss: 0.6189 - val_accuracy: 0.8000\n",
      "Epoch 62/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.3013 - accuracy: 0.8663 - val_loss: 0.5053 - val_accuracy: 0.8250\n",
      "Epoch 63/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3033 - accuracy: 0.8564 - val_loss: 0.6067 - val_accuracy: 0.7500\n",
      "Epoch 64/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.3207 - accuracy: 0.8651 - val_loss: 0.5022 - val_accuracy: 0.7750\n",
      "Epoch 65/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3015 - accuracy: 0.8750 - val_loss: 0.6821 - val_accuracy: 0.7750\n",
      "Epoch 66/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2954 - accuracy: 0.8725 - val_loss: 0.5754 - val_accuracy: 0.8250\n",
      "Epoch 67/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2948 - accuracy: 0.8812 - val_loss: 0.5686 - val_accuracy: 0.8000\n",
      "Epoch 68/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2953 - accuracy: 0.8812 - val_loss: 0.5480 - val_accuracy: 0.7750\n",
      "Epoch 69/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.3007 - accuracy: 0.8725 - val_loss: 0.6302 - val_accuracy: 0.8000\n",
      "Epoch 70/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2912 - accuracy: 0.8861 - val_loss: 0.4109 - val_accuracy: 0.8500\n",
      "Epoch 71/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2978 - accuracy: 0.8738 - val_loss: 0.4132 - val_accuracy: 0.8250\n",
      "Epoch 72/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2778 - accuracy: 0.8886 - val_loss: 0.4485 - val_accuracy: 0.8250\n",
      "Epoch 73/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2947 - accuracy: 0.8800 - val_loss: 0.5276 - val_accuracy: 0.8000\n",
      "Epoch 74/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2745 - accuracy: 0.8874 - val_loss: 0.5332 - val_accuracy: 0.7500\n",
      "Epoch 75/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2901 - accuracy: 0.8750 - val_loss: 0.5836 - val_accuracy: 0.7750\n",
      "Epoch 76/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2881 - accuracy: 0.8738 - val_loss: 0.6802 - val_accuracy: 0.8000\n",
      "Epoch 77/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2927 - accuracy: 0.8899 - val_loss: 0.5233 - val_accuracy: 0.8000\n",
      "Epoch 78/150\n",
      "202/202 [==============================] - 85s 419ms/step - loss: 0.2850 - accuracy: 0.8874 - val_loss: 0.4344 - val_accuracy: 0.8500\n",
      "Epoch 79/150\n",
      "202/202 [==============================] - 92s 456ms/step - loss: 0.2649 - accuracy: 0.8874 - val_loss: 0.5854 - val_accuracy: 0.7750\n",
      "Epoch 80/150\n",
      "202/202 [==============================] - 85s 419ms/step - loss: 0.2567 - accuracy: 0.9010 - val_loss: 0.3153 - val_accuracy: 0.8500\n",
      "Epoch 81/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2776 - accuracy: 0.8936 - val_loss: 0.5010 - val_accuracy: 0.8000\n",
      "Epoch 82/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2818 - accuracy: 0.8874 - val_loss: 0.4843 - val_accuracy: 0.8750\n",
      "Epoch 83/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2654 - accuracy: 0.8911 - val_loss: 0.4199 - val_accuracy: 0.8500\n",
      "Epoch 84/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2623 - accuracy: 0.9035 - val_loss: 0.5143 - val_accuracy: 0.8000\n",
      "Epoch 85/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2615 - accuracy: 0.8911 - val_loss: 0.4518 - val_accuracy: 0.8250\n",
      "Epoch 86/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2676 - accuracy: 0.8812 - val_loss: 0.5840 - val_accuracy: 0.7500\n",
      "Epoch 87/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2516 - accuracy: 0.9084 - val_loss: 0.5602 - val_accuracy: 0.7500\n",
      "Epoch 88/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2519 - accuracy: 0.8948 - val_loss: 0.5697 - val_accuracy: 0.8250\n",
      "Epoch 89/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2515 - accuracy: 0.8948 - val_loss: 0.6231 - val_accuracy: 0.7750\n",
      "Epoch 90/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2787 - accuracy: 0.8936 - val_loss: 0.4926 - val_accuracy: 0.8250\n",
      "Epoch 91/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2404 - accuracy: 0.9022 - val_loss: 0.6034 - val_accuracy: 0.8250\n",
      "Epoch 92/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2535 - accuracy: 0.9022 - val_loss: 0.6673 - val_accuracy: 0.8000\n",
      "Epoch 93/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2406 - accuracy: 0.9134 - val_loss: 0.5930 - val_accuracy: 0.7500\n",
      "Epoch 94/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2512 - accuracy: 0.8960 - val_loss: 0.5545 - val_accuracy: 0.8250\n",
      "Epoch 95/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.2666 - accuracy: 0.8886 - val_loss: 0.5900 - val_accuracy: 0.8500\n",
      "Epoch 96/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2390 - accuracy: 0.8985 - val_loss: 0.7308 - val_accuracy: 0.8000\n",
      "Epoch 97/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2414 - accuracy: 0.8948 - val_loss: 0.5174 - val_accuracy: 0.8000\n",
      "Epoch 98/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2568 - accuracy: 0.8998 - val_loss: 0.3986 - val_accuracy: 0.8250\n",
      "Epoch 99/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2347 - accuracy: 0.9035 - val_loss: 0.6744 - val_accuracy: 0.7500\n",
      "Epoch 100/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2364 - accuracy: 0.9109 - val_loss: 0.6650 - val_accuracy: 0.7750\n",
      "Epoch 101/150\n",
      "202/202 [==============================] - 82s 404ms/step - loss: 0.2461 - accuracy: 0.9010 - val_loss: 0.5451 - val_accuracy: 0.8250\n",
      "Epoch 102/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2278 - accuracy: 0.9097 - val_loss: 0.7194 - val_accuracy: 0.7750\n",
      "Epoch 103/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2417 - accuracy: 0.9047 - val_loss: 0.6431 - val_accuracy: 0.7250\n",
      "Epoch 104/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2134 - accuracy: 0.9134 - val_loss: 0.6618 - val_accuracy: 0.8000\n",
      "Epoch 105/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2603 - accuracy: 0.8874 - val_loss: 0.3764 - val_accuracy: 0.8250\n",
      "Epoch 106/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.2212 - accuracy: 0.9097 - val_loss: 0.8001 - val_accuracy: 0.7750\n",
      "Epoch 107/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2376 - accuracy: 0.9047 - val_loss: 0.6069 - val_accuracy: 0.7750\n",
      "Epoch 108/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2472 - accuracy: 0.9010 - val_loss: 0.6138 - val_accuracy: 0.8250\n",
      "Epoch 109/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2318 - accuracy: 0.8948 - val_loss: 0.7585 - val_accuracy: 0.7500\n",
      "Epoch 110/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2256 - accuracy: 0.9121 - val_loss: 0.7343 - val_accuracy: 0.7750\n",
      "Epoch 111/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2398 - accuracy: 0.9022 - val_loss: 0.6233 - val_accuracy: 0.8000\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2273 - accuracy: 0.9121 - val_loss: 0.5451 - val_accuracy: 0.8000\n",
      "Epoch 113/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2166 - accuracy: 0.9183 - val_loss: 0.6546 - val_accuracy: 0.7500\n",
      "Epoch 114/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2297 - accuracy: 0.9109 - val_loss: 0.7528 - val_accuracy: 0.7500\n",
      "Epoch 115/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2094 - accuracy: 0.9183 - val_loss: 0.4894 - val_accuracy: 0.8250\n",
      "Epoch 116/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2411 - accuracy: 0.8998 - val_loss: 0.6709 - val_accuracy: 0.7750\n",
      "Epoch 117/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2281 - accuracy: 0.9084 - val_loss: 0.6947 - val_accuracy: 0.7750\n",
      "Epoch 118/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2138 - accuracy: 0.9208 - val_loss: 0.7556 - val_accuracy: 0.8000\n",
      "Epoch 119/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2288 - accuracy: 0.9084 - val_loss: 0.6328 - val_accuracy: 0.8250\n",
      "Epoch 120/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1989 - accuracy: 0.9158 - val_loss: 0.6258 - val_accuracy: 0.7750\n",
      "Epoch 121/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2195 - accuracy: 0.9097 - val_loss: 0.4429 - val_accuracy: 0.8500\n",
      "Epoch 122/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2332 - accuracy: 0.9035 - val_loss: 0.7103 - val_accuracy: 0.7750\n",
      "Epoch 123/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2231 - accuracy: 0.8985 - val_loss: 0.6649 - val_accuracy: 0.8000\n",
      "Epoch 124/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2206 - accuracy: 0.9183 - val_loss: 0.7244 - val_accuracy: 0.7250\n",
      "Epoch 125/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2168 - accuracy: 0.9109 - val_loss: 0.6922 - val_accuracy: 0.7750\n",
      "Epoch 126/150\n",
      "202/202 [==============================] - 81s 400ms/step - loss: 0.2293 - accuracy: 0.9035 - val_loss: 0.4758 - val_accuracy: 0.8250\n",
      "Epoch 127/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2266 - accuracy: 0.9134 - val_loss: 0.7463 - val_accuracy: 0.7500\n",
      "Epoch 128/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2245 - accuracy: 0.9183 - val_loss: 0.5591 - val_accuracy: 0.7750\n",
      "Epoch 129/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2077 - accuracy: 0.9183 - val_loss: 0.7486 - val_accuracy: 0.7500\n",
      "Epoch 130/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1956 - accuracy: 0.9220 - val_loss: 0.6362 - val_accuracy: 0.8000\n",
      "Epoch 131/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1958 - accuracy: 0.9146 - val_loss: 0.7834 - val_accuracy: 0.7500\n",
      "Epoch 132/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2059 - accuracy: 0.9158 - val_loss: 0.5716 - val_accuracy: 0.8000\n",
      "Epoch 133/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2049 - accuracy: 0.9158 - val_loss: 0.7307 - val_accuracy: 0.7750\n",
      "Epoch 134/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1872 - accuracy: 0.9257 - val_loss: 0.7550 - val_accuracy: 0.8000\n",
      "Epoch 135/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1976 - accuracy: 0.9257 - val_loss: 0.6655 - val_accuracy: 0.7750\n",
      "Epoch 136/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.2043 - accuracy: 0.9022 - val_loss: 0.7957 - val_accuracy: 0.7750\n",
      "Epoch 137/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1967 - accuracy: 0.9208 - val_loss: 0.7293 - val_accuracy: 0.7500\n",
      "Epoch 138/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2167 - accuracy: 0.9158 - val_loss: 0.7164 - val_accuracy: 0.7500\n",
      "Epoch 139/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.2178 - accuracy: 0.9146 - val_loss: 0.6981 - val_accuracy: 0.8250\n",
      "Epoch 140/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.1858 - accuracy: 0.9183 - val_loss: 0.5335 - val_accuracy: 0.8250\n",
      "Epoch 141/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2104 - accuracy: 0.9183 - val_loss: 0.5743 - val_accuracy: 0.8000\n",
      "Epoch 142/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1952 - accuracy: 0.9109 - val_loss: 0.5753 - val_accuracy: 0.8000\n",
      "Epoch 143/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1972 - accuracy: 0.9183 - val_loss: 0.7716 - val_accuracy: 0.7500\n",
      "Epoch 144/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.1830 - accuracy: 0.9208 - val_loss: 0.6827 - val_accuracy: 0.8000\n",
      "Epoch 145/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2090 - accuracy: 0.9072 - val_loss: 0.7587 - val_accuracy: 0.8000\n",
      "Epoch 146/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.1969 - accuracy: 0.9158 - val_loss: 0.7641 - val_accuracy: 0.8000\n",
      "Epoch 147/150\n",
      "202/202 [==============================] - 81s 402ms/step - loss: 0.2025 - accuracy: 0.9183 - val_loss: 0.6228 - val_accuracy: 0.8000\n",
      "Epoch 148/150\n",
      "202/202 [==============================] - 82s 403ms/step - loss: 0.1979 - accuracy: 0.9196 - val_loss: 0.7784 - val_accuracy: 0.7750\n",
      "Epoch 149/150\n",
      "202/202 [==============================] - 81s 401ms/step - loss: 0.1829 - accuracy: 0.9233 - val_loss: 0.8134 - val_accuracy: 0.7500\n",
      "Epoch 150/150\n",
      "202/202 [==============================] - 82s 405ms/step - loss: 0.1909 - accuracy: 0.9183 - val_loss: 0.8582 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x214d6ee8e80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=150, validation_data=validation_ds, validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d535a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 66ms/step - loss: 0.8974 - accuracy: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8974118828773499, 0.7954545617103577]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf3c78af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.7946423e-01]\n",
      " [8.8307303e-01]\n",
      " [2.3773313e-04]\n",
      " [1.9580373e-01]\n",
      " [1.8496885e-05]\n",
      " [2.1280050e-03]\n",
      " [9.2077279e-01]\n",
      " [5.7686478e-02]\n",
      " [9.8579621e-01]\n",
      " [1.5324354e-04]\n",
      " [9.9782127e-01]\n",
      " [1.2836695e-02]\n",
      " [9.8657161e-01]\n",
      " [1.0122955e-03]\n",
      " [8.8411778e-01]\n",
      " [1.9379062e-05]\n",
      " [9.9637687e-01]\n",
      " [7.2208756e-01]\n",
      " [9.9479669e-01]\n",
      " [9.4961476e-01]\n",
      " [9.7185373e-01]\n",
      " [8.8845611e-01]\n",
      " [9.9979389e-01]\n",
      " [9.9866062e-01]\n",
      " [3.3811969e-01]\n",
      " [9.4742697e-01]\n",
      " [7.9345324e-07]\n",
      " [9.9901468e-01]\n",
      " [5.1561755e-01]\n",
      " [1.0665892e-05]\n",
      " [1.0505199e-02]\n",
      " [3.4149000e-06]\n",
      " [8.7798858e-01]\n",
      " [8.0761909e-03]\n",
      " [8.4111074e-05]\n",
      " [3.0475855e-04]\n",
      " [2.7874197e-05]\n",
      " [4.4505805e-02]\n",
      " [9.8368025e-01]\n",
      " [1.1101030e-10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true=np.concatenate([y for x, y in validation_ds], axis=0)\n",
    "X_test=np.concatenate([x for x, y in validation_ds], axis=0)\n",
    "y_pred=model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db340b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b6f6534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAF8CAYAAAAuF9n2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA+Q0lEQVR4nO3deZyNdf/H8dfHMMYybkILLXJbijK2UJGlRdHCLd32JSIhUndKN4nqVndSd6khWZP2ojtbliwhSxkptFFkyXaTvZn5/v44R79pmhlnOOdcc855Px+P83DOdV3nOu+5DJ/z/V7f63uZcw4RERGJDfm8DiAiIiLho8IvIiISQ1T4RUREYogKv4iISAxR4RcREYkhKvwiIiIxJKyF38zGm9kvZrY+m/VmZv8xs+/MbJ2Z1QxnPhERkWgX7hb/RODGHNbfBFT0P3oAL4chk4iISMwIa+F3zi0G9uWwyW3AZOezAihuZueFJ52IiEj0y2vn+MsCWzO83uZfJiIiIkGQ3+sAmVgWy7KcU9jMeuA7HUCRIkVqXXLJJaHMJSIiedSXPx/wOkJYuLRUUg/swp04Sr6ERNKP/brHOVc6t/vJa4V/G3BBhtfnA9uz2tA5NxYYC1C7dm23evXq0KcTEZE8p9xDHwGwZURzj5OEzrZt26hevTrHCuRj9NiJdOrUiXz58v14OvvKa139M4BO/tH99YADzrkdXocSERHxwskb6ZUtW5bevXuzZs0aOnfujFlWHeSBCfflfNOA5UBlM9tmZt3M7G4zu9u/yUzgB+A74BXgnnDmExERySs2bdpEgwYN2LBhA2bGY489RuXKlc94v2Ht6nfOtT3Fegf0DlMcERGRPMc5x6RJk+jTpw8JCQns3LmTSy+9NGj7z2td/SIiIjHr4MGDdOjQga5du3LFFVeQkpJC48aNg/oZKvwiIiJ5xLPPPsubb77J8OHDmTdvHmXLBv+K9rw2ql9ERCSmpKens3PnTsqUKcNDDz1Es2bNqFOnTsg+Ty1+ERERj+zatYvmzZvToEEDDh06REJCQkiLPqjFLyIi4omPP/6Yjh078r///Y9Ro0ZRpEiRsHyuWvwiIiJhlJqaysMPP0zTpk0566yzWLVqFb169Tqja/NzQ4VfREQkjMyMZcuW0b17d1avXs3ll18e1s9XV7+IiEgYvPPOOzRo0IBzzjmHOXPmkJCQ4EkOtfhFRERC6PDhw3Tv3p3WrVvzzDPPAHhW9EEtfhERkZBZt24df//739m0aRODBg1i6NChXkdS4RcREQmFWbNm0bJlS0qUKMHHH3/Mtdde63UkQF39IiIiIVGnTh3atm1LSkpKnin6oBa/iEhU6TphJQs37fY6RsxasmQJzz//PNOmTaNkyZJMmDDB60h/oha/iEgUidWi37hyaU8/Py0tjWHDhtGoUSNSUlL4+eefPc2TE7X4RUSi0JYRzb2OEDO2bdtG+/btWbx4MR06dOCll14iMTHR61jZUuEXERE5A23atGHt2rVMmjSJTp06eR3nlFT4RUREcunYsWOkp6dTuHBhxowZQ4ECBahUqZLXsQKic/wiIiK5sGnTJurVq8e9994LQNWqVSOm6IMKv4iISECcc0yYMIGaNWuybds2WrRo4XWk06LCLyIicgoHDx6kffv23HnnndSpU4eUlBRuvvlmr2OdFhV+ERGRU9izZw9z5szh8ccfZ968eZQtW9brSKdNg/tERESykJ6ezvvvv8/f/vY3ypcvz/fff0/x4sW9jnXG1OIXERHJZNeuXTRr1ozbb7+dmTNnAkRF0Qe1+EVERP7g448/pmPHjhw4cIDk5GSaNWvmdaSgUotfRETE76mnnuKGG26gZMmSrFq1ip49e2JmXscKKhV+ERERvxo1atCjRw9WrVrFZZdd5nWckFBXv4iIxLQ333yTrVu38sADD3DDDTdwww03eB0ppNTiFxGRmHT48GG6d+9OmzZtmDFjBqmpqV5HCgsVfhERiTnr1q2jdu3ajB8/nkGDBjF//nzy54+NTvDY+ClFRET89u3bx9VXX01iYiLz5s2jSZMmXkcKKxV+ERGJCUePHqVQoUKcddZZTJo0iQYNGlC6dGmvY4WduvpFRCTqLVmyhMqVKzNjxgwA/va3v8Vk0QcVfhERiWJpaWk89thjNGrUiIIFC0b0HPvBoq5+ERGJStu2baN9+/YsXryYjh07Mnr0aBITE72O5TkVfhERiUrz5s1jzZo1TJo0iU6dOnkdJ89QV7+IiESNY8eOsXz5cgA6d+7MN998o6KfiQq/iIhEhY0bN1KvXj2uv/569uzZg5lRpkwZr2PlOSr8IiIS0ZxzTJgwgVq1avHzzz/z1ltvUapUKa9j5Vkq/CIiErHS0tLo0KEDd955J3Xr1iUlJSXqbqMbbCr8IiISseLi4ihdujSPP/44H3/8sbr2A6BR/SIiElHS09MZOXIkjRo14oorruC5557zOlJEUYtfREQixq5du7jpppt48MEHmTZtmtdxIpJa/CIiEhHmzp1Lx44dOXjwIGPGjOGuu+7yOlJEUuEXEZE87+OPP6Zp06ZUrVqVBQsWULVqVa8jRSx19YuISJ6VlpYGQOPGjXnmmWdYtWqViv4ZUuEXEZE86Y033qBKlSrs2rWL/Pnzc//991OoUCGvY0U8FX4REclTDh8+TLdu3Wjbti2lSpXit99+8zpSVFHhFxGRPGPt2rXUqlWLCRMm8Mgjj7Bo0SLOP/98r2NFFQ3uE5Go1XXCShZu2u11DMmFJ598koMHDzJv3jyaNGnidZyopMIvIlErVot+48qlvY6QK3v37uXIkSNccMEFJCcnk5aWRunSkfUzRBIVfhGJeltGNPc6gmRj0aJFtG/fnvLly7No0SLOOussryNFPZ3jFxGRsEtNTWXo0KE0adKEQoUKMWrUKMzM61gxQS1+EREJq127dnHHHXewePFiOnbsyOjRo0lMTPQ6VsxQ4RcRkbAqWrQoR44cYfLkyXTs2NHrODFHXf0iIhJyx44dY/jw4Rw+fJgiRYrw2Wefqeh7RIVfRERCasOGDdStW5chQ4Ywc+ZMAPLlU/nxStiPvJndaGabzOw7M3soi/V/MbMPzSzFzL4ys67hzigiImfOOcerr75K7dq12bFjBx999BGtW7f2OlbMC2vhN7M4YDRwE1AFaGtmVTJt1hv42jmXBDQCRppZfDhziojImRs+fDjdu3fnyiuvJCUlhWbNmnkdSQj/4L46wHfOuR8AzOwN4Dbg6wzbOCDRfNd1FAX2AalhzikiIqfJOYeZ0aFDBxISErj//vuJi4vzOpb4hburvyywNcPrbf5lGb0IXApsB74E+jnn0sMTT0RETld6ejpPP/00bdq0wTlH+fLlefDBB1X085hwF/6sZmdwmV43BdYCZYDqwItmVuxPOzLrYWarzWz17t2xOS2niEhesXPnTm688UYGDhxIeno6x48f9zqSZCPchX8bcEGG1+fja9ln1BV4z/l8B2wGLsm8I+fcWOdcbedcbc3pLCLinTlz5pCUlMSSJUsYM2YMb731FgkJCV7HkmyE+xz/KqCimV0M/Ay0Adpl2uYn4FpgiZmdA1QGfghrShERCciRI0fo3LkzpUuXZsGCBVStWtXrSHIKYS38zrlUM+sDzAHigPHOua/M7G7/+mRgODDRzL7Ed2pgoHNuTzhziohIzn766SfKli1L4cKFmTt3LhUrVqRQoUJex5IAhP06fufcTOdcJefcX51zT/iXJfuLPs657c65G5xzlzvnLnPOvRbujCIikr1p06Zx2WWX8fTTTwNQrVo1Ff0IoqmTREQkIIcPH+bOO++kXbt2XH755bRrl/lMrUQCFX4RETmldevWUatWLSZOnMgjjzzCokWLuOiii7yOJadBd+cTEZFTOnr0KEePHmXevHk0adLE6zhyBtTiFxGRLO3du5fx48cDULduXb799lsV/Sigwi8iIn+yaNEikpKS6NWrFz/++CMA8fG6bUo0UOEXEZHfpaam8uijj9KkSRMKFy7M8uXLdS4/yugcv4iIAL6b69xyyy3Mnj2bzp0788ILL5CYmOh1LAkyFX4REQHAzGjXrh0dOnSgffv2XseREFHhFxGJYceOHeOBBx6gdu3adOnShY4dO3odSUJM5/hFRGLUhg0bqFu3LqNHj+b777/3Oo6EiVr8IiIxxjnH+PHjuffeeylSpAgzZ87kpptu8jqWhIla/CIiMWbVqlV0796dK6+8kpSUFBX9GKMWv4hIjNi9ezelS5emTp06zJ49m+uuu464uDivY0mYqcUvIhLl0tPTGTFiBOXKlWP16tUANG3aVEU/RqnFLyISxXbu3EnHjh2ZN28et99+OxUqVPA6knhMLX4RkSg1Z84ckpKSWLp0KWPGjOGtt96iePHiXscSj6nFLyISpZYtW8bZZ5/NggULqFq1qtdxJI9Qi19EJIp8//33LFu2DIDBgwezcuVKFX35AxV+EZEo8frrr1OjRg26d+9Oeno6+fPnp1ChQl7HkjxGXf0iclq6TljJwk27vY4hwKFDh+jbty8TJ07k6quv5vXXXydfPrXrJGsq/CJyWiKl6DeuXNrrCCH1yy+/0KBBA7799lsGDx7MkCFDyJ9f/7VL9vTbISJnZMuI5l5HiGmlS5emcePGjBkzhkaNGnkdRyKA+oJERCLM3r176dChA5s3b8bMSE5OVtGXgOW68JtZUTO7yMwKhCKQiIhkb9GiRSQlJfH222//PgufSG4EXPjN7GYz+xw4AHwPXO5fPs7M2oUon4iIAKmpqTz66KM0adKEwoULs2LFClq3bu11LIlAARV+M2sBTAf2AAMzvW8z0DnoyURE5HfPPPMMw4YNo2PHjnz++efUqFHD60gSoQId3PcoMME5193M8gNPZ1i3Hrgn6MlERIRDhw5RtGhR+vTpQ8WKFWnVqpXXkSTCBdrVfynwpv+5y7RuP1AyaIlERISjR4/Su3dv6taty5EjRyhatKiKvgRFoIX/IFAqm3XlgMi4oFdEJAJ8/fXX1K1bl5deeokbb7xR1+VLUAVa+D8GHjaz4hmWOTMrCPQBZgU7mIhIrHHOMW7cOGrXrs3OnTuZOXMmI0eOJD4+3utoEkUC/Rr5CLAS2ATMxNfd/xBQDfgL0CIU4UREYklqaipjx47lqquuYsqUKZx33nleR5IoFFCL3zm3BagJ/Be4HkgDrgFWAHWdc9tDFVBEJNp99tln7Nu3jwIFCjBz5kzmzp2roi8hE/B1/M65bc65bs65851z8c6585xzXZ1zW0MZUEQkWqWnpzNixAiuvvpq/vnPfwJQqlQp3WBHQirQ6/gXmNkl2ayrZGYLghtLRCS67dixgxtuuIGHH36YVq1a8eSTT3odSWJEoOf4GwHFslmXCDQMShoRkRiwYsUKbr31Vg4dOsQrr7xCt27dMDOvY0mMyM01Ipmv3z/pr8ChIGQREYkJ5cqVo1q1avznP/+hSpUqXseRGJNt4TezrkBX/0sHjDWzXzNtVgi4DJgfmngiItHh+++/5/nnn2fUqFGce+65zJs3z+tIEqNyavGn4xu9D2CZXp+0F3gZeCr40URiW9cJK1m4SXNjRYOpU6fSq1cv4uLi6NWrF5deeqnXkSSGZVv4nXOTgEkAZrYQ6OWc2xiuYCKxLhKKfuPKpb2OkKcdOnSIPn36MGnSJOrXr8/UqVO58MILvY4lMS6gc/zOucahDiIiWdsyornXEeQ0tW7dmjlz5jBkyBAGDx6sqXclT8jVb6GZJQGVgYTM65xzk4MVSkQkUjnnSE1NpUCBAjz22GMMHDiQRo0aeR1L5HcBFX7/HP0fAfVOLvL/mXGkvwq/iMS0PXv20LVrV/7617/y3HPPUadOHa8jifxJoNNDPYnv1rvX4Cv6LYEmwFTgB0C/3SIS0z755BOSkpKYO3cu5cuX9zqOSLYCLfxN8RX/Ff7X25xznzjnOgHzgH6hCCciktelpqYyZMgQmjRpQtGiRVmxYgX33nuv17FEshVo4T8P+ME5lwYcwzdb30nvARp9JCIxacuWLTzzzDN07tyZNWvWUKNGDa8jieQo0MF9O4Hi/uc/AlcCn/hfVwhuJBGRvG/lypVcccUVVKhQgfXr16t7XyJGoC3+pfiKPcAU4FEzG2Nmo4F/A3NCEU5EJK85evQovXr1om7dunzwwQcAKvoSUQJt8T8GlPE//ze+gX5/BwoDM4C+wY8mIpK3fPXVV7Rp04b169fzwAMP0Ly5znJK5Al0Ap/vge/9z38D7vc/RERiwpQpU+jZsydFixZl1qxZ3HjjjV5HEjktgXb1Z8vMapjZ+8EIIyKSVyUmJlK/fn3WrVunoi8RLcfCb2ZxZlbHzG43sxqZ1tU2sw+B1YCm9BWRqLN8+XLGjx8PQIsWLZgzZw7nnnuux6lEzky2hd/Mzgc+A5YDbwGrzexNM4s3s3H+dU2AkYBGtohI1EhPT+df//oXDRo04KmnnuLEiRMAmNkp3imS9+V0jn8EcAkwGPgcuBgYBHwK1MJ3576HnHO7Qh1SRCRcduzYQceOHZk/fz533HEHY8aMIT4+3utYIkGTU+G/FhjqnHvm5AIz24Rvpr4XnHOarU9Eosqvv/5KzZo1OXDgAOPGjePOO+9UK1+iTk6FvzT/P0XvScv9f74dmjgiIuGXnp5Ovnz5SExMZOjQoTRo0IAqVap4HUskJHIa3JcPOJFp2cnXR0ITR0QkvL777jvq1avHvHnzAOjZs6eKvkS1U13Hf4uZXZbhdT58t+K91cyqZ9zQOTc+yNlERELqtddeo1evXhQoUIDjx497HUckLE5V+B/JZvmQTK8dEFDhN7MbgeeBOGCcc25EFts0Ap4DCgB7nHMNA9m3iEggDh06RJ8+fZg0aRL169dn6tSpXHjhhV7HEgmLnAr/xcH+MDOLA0YD1wPbgFVmNsM593WGbYoDLwE3Oud+MrOzg51DRGLbW2+9xZQpUxgyZAiDBw8mf/5AZy8XiXzZ/rY7534MwefVAb5zzv0AYGZvALcBX2fYph3wnnPuJ3+OX0KQQ0RijHOOb7/9lkqVKtG1a1dq1apFUlKS17FEwu6Mp+zNpbLA1gyvt/mXZVQJKGFmn5jZGjPrlNWOzKyHma02s9W7d+8OUVwRiQa7d+/mlltuoU6dOuzYsQMzU9GXmBXu/q2sLoh1mV7nxzdB0LVAIWC5ma1wzn3zhzc5NxYYC1C7du3M+xARAWDhwoW0b9+evXv38swzz2jKXYl54W7xbwMuyPD6fGB7FtvMds4dds7tARYD+mouIrninGPw4MFce+21FCtWjM8++4y+fftqQh6JeeEu/KuAimZ2sZnFA22AGZm2mQ40MLP8ZlYYqAtsCHNOEYlwZsa2bdvo0qULa9asoXr16l5HEskTwtrV75xLNbM+wBx8l/ONd859ZWZ3+9cnO+c2mNlsYB2Qju+Sv/XhzCkikeu9996jYsWKXH755bzyyisasS+SSa7+RZhZPqAKUBJY7Zw7nNsPdM7NBGZmWpac6fW/gX/ndt8iEruOHj3KgAEDSE5OpkOHDkyZMkVFXyQLAXf1m1lvYCeQAiwAKvuXf2Bm94YmnojIqX311VfUqVOH5ORk/vGPf/Dqq696HUkkzwqo8JvZXfhm2/sA+Dt/HJ2/BGgV9GQiIgFYsWIFV1xxBb/88guzZ8/m6aef1m10RXIQaIt/ADDSOdcDeD/Tuo34W/8iIuFWs2ZNevbsSUpKCk2bNvU6jkieF2jhvxjfgLysHAaKByWNiEgAli9fTsOGDdm/fz/x8fGMGjVK1+eLBCjQwr8HKJfNusrAz0FJIyKSg7S0NJ588kkaNGjATz/9xPbtmacBEZFTCbTwfwgMMbPyGZY5MysF3Ifv3L+ISMjs2LGDpk2b8sgjj3D77bezdu1aqlat6nUskYgTaOH/J3AcWA/MwzfN7n/wTayTBgwLSToREb/77ruPZcuWMW7cOKZNm8Zf/vIXryOJRKSACr9zbi9QG/gXUAD4Ht8cAC8CVzrnDoQsoYjErBMnTrBnzx4ARo0axZo1a+jWrZum3RU5AwHPbuGc+xUY7n+IiITUt99+S9u2bSlWrBjz58/nvPPO47zzzvM6lkjEC/Q6/mfNrHqIs4iIADBlyhRq1qzJDz/8wL333qsWvkgQBXqOvyuwxszWm9k/zKxsKEOJSGw6dOgQnTp1olOnTtSoUYOUlBRatGjhdSyRqBJo4T8HuAP4Dl9X/49mNs/MOppZkZClE5GYkpqayrJlyxg6dCgLFizgggsuOPWbRCRXAh3cd8I5965zrgVwHnAvUAiYBOwysymhiygi0cw5x+TJkzl+/DjFixfnyy+/5NFHH9UNdkRCJOCb9JzknNvvnHvJOXc10BjYD7QLejIRiXq7d+/mlltuoXPnzkyZ4ms/FCpUyONUItEt11+p/V37twMdgEZAKvBucGOJSLRbuHAh7du3Z+/evfznP/+hW7duXkcSiQmBjurPZ2Y3mtlUYBcwHigI3AOc65y7I4QZRSTKJCcnc+2111KsWDE+++wz+vbtq5H7ImESaIt/O1Aa3+C+p4ApzrktoQolItHtmmuuoUePHowcOZIiRTQ+WCScAj3H/y5wlXOusnNuuIq+iOTWu+++S58+fQCoUqUKycnJKvoiHgh0VH9v59xnoQ4jItHn6NGj3H333dx+++2sXLmSQ4cOeR1JJKZl29VvZtcAnzvnDvmf58g5tzioyUQk4n311Ve0adOG9evX8+CDDzJ8+HDi4+O9jiUS03I6x/8JUA9Y6X/ustnO/OvighlMRCLb8ePHadq0Kb/99huzZ8+madOmXkcSEXIu/I2Br/3Pm5B94RcR+d3BgwcpWrQoBQsW5PXXX6dSpUqce+65XscSEb9sC79zblGG55+EJY2IRLRly5bRtm1b+vXrx4ABA7jmmlOeJRSRMAv0Ov4fzCwpm3WXmdkPwY0lIpEkLS2NJ554gmuuuYb8+fNTv359ryOJSDYCvY6/HL4Je7KSAFwUlDQiEnG2b99Ohw4dWLhwIW3btuXll1/mL3/5i9exRCQbuZmyN7tz/LWB/515FBGJRN9++y2rV69m/PjxdOnSRTPwieRxOV3Odx9wn/+lAz40sxOZNisEnAW8EZp4IpIXHT9+nPnz59OsWTMaNmzIjz/+SIkSJbyOJSIByKnF/wMw3/+8M7Aa2J1pm+P4Rv6PC340EcmLvv32W9q0acPatWvZsGEDlSpVUtEXiSA5jeqfDkwHTnbdDXPObQ5TLhHJg6ZMmcI999xDfHw87733HpUqVfI6kojkUqBT9nZV0ReJbXfddRedOnWiZs2arF27lttuu83rSCJyGnI6xz8EGOec2+5/nhPnnBse3GgikpckJSUxdOhQ/vnPfxIXp4k6RSJVTuf4hwKz8d2Sd+gp9uMAFX6RKOKc47nnnuPCCy+kVatWv99ZT0QiW7Zd/c65fM65lRme5/TQ13+RKLJ7925uvvlmBgwYwIwZM7yOIyJBFNA5fhGJHQsWLCApKYn58+fzwgsvMHHiRK8jiUgQBTSBj5lVAoqf7AEws0LAEOAyYI5z7sXQRRSRcFm3bh3XXXcdlStXZtasWSQlZTlTt4hEsEBb/C8Ct2d4/QRwP1AGGGVmvYMdTETC59ixYwBUq1aNV199ldWrV6voi0SpQAt/NeBTADPLB3QCBjrnagGPAz1CE09EQu2dd96hfPnyrF+/HoCuXbtSpEgRj1OJSKgEWviLA3v9z2sAJYB3/K8/AcoHNZWIhNyRI0fo2bMnrVu35oILLlCxF4kRgRb+XUAF//MbgO+dc1v9r4sCqcEOJiKhs379eurUqcPYsWMZOHAgS5cu5eKLL/Y6loiEQaB355sB/MvMLgO6AGMyrLsc37z+IhIhXnvtNfbs2cPcuXO5/vrrvY4jImEUaIv/IeC/QFN8XwKezLDuVmBukHOJSJDt37//9/P4w4YNY926dSr6IjEooBa/c+4wcFc2664KaiIRCbpPP/2Udu3aER8fz4YNG4iPj+fss8/2OpaIeCBXE/iY2Vlm1tzMOppZMzM7K1TBROTMpaWl8fjjj9OwYUPy58/P1KlTyZ8/0DN8IhKNAv4fwMwex3ftfsEMi4+b2TPOucFBTyYiZ+TAgQO0bNmShQsX0rZtW5KTkylWrJjXsUTEY4HO3NcfGAS8CrwG7ATOBToAg8xst3PuP6EKKZGn64SVLNy02+sYMS0xMZFixYoxfvx4unTpgpl5HUlE8oBAW/x3A8875+7LsGwTsMjMDgH3ACr88jsV/eBoXLl0rrY/fvw4w4YNo3fv3pQpU4b3339fBV9E/iDQwl8O+CibdR8BvYKSRqLOlhHNvY4QM7755hvatm3L559/zvnnn0+vXr1U9EXkTwId3LcX3w15slKV/5/VT0Q8MHnyZGrWrMmWLVv44IMP6NVL38VFJGuBFv73geH+0fwFAMwsv5m1BYYB74YqoIjk7KWXXqJz587UqlWLlJQUbrvtNq8jiUgeFmhX/8NAEjAJGG9m+4CzgDhgKb6BfyISRmlpacTFxdGuXTuOHTtGv379iIuL8zqWiORxAbX4nXO/Atfgm6XvWXyz9z0L3Aw0dM4dCllCEfmD9PR0nn32WerXr8/x48cpXrw4AwYMUNEXkYDk2OI3s1L4LtmrAOwH3nXODQxHMBH5s19++YUuXbowa9YsWrRowbFjxyhYsOCp3ygi4pdt4TezysBiIOP1RA+Z2e3OuekhTyYifzB//nw6dOjA/v37GT16tEbti8hpyamr/3HgGNAIKILvLnwr8XXxi0gYpaWlcf/991OiRAlWrlzJPffco6IvIqclp67+usBg59xi/+uvzKwnkGJmpZ1zmqFFJMR+/PFHzjrrLBITE5k+fTqlSpWiSJEiXscSkQiWU4u/LL7Z+TLaBBhQJmSJRASAt99+m6SkJB544AEALrroIhV9ETljORV+A9IyLUsP4H05MrMbzWyTmX1nZg/lsN0VZpZmZref7meJRKIjR47Qs2dP7rjjDi655BIeeijbfyYiIrl2quv4HzOzPRlenzypONx/Lf9JzjnX+VQfZmZxwGjgemAbsMrMZjjnvs5iu6eAOafap0g02bhxI61ateLrr79m4MCBDB8+nAIFCngdS0SiSE6F/yfg0iyW/4hvmt6MXICfVwf4zjn3A4CZvQHcBnydabu++GYDvCLA/YpEhUKFCuGcY+7cuVx//fVexxGRKJRt4XfOlQvB55UFtmZ4vQ3fIMLfmVlZoCXQhBwKv5n1AHoAXHjhhUEPKhIu+/fvZ+zYsTz44INcdNFFrF+/nnz5TvtsmohIjsL9v0tW1x9l7i14DhjonMs8vuCPb3JurHOutnOudunSubt1qUhe8emnn1K9enX++c9/smbNGgAVfREJqXD/D7MNuCDD6/OB7Zm2qQ28YWZbgNuBl8ysRVjSiYRJWloajz/+OA0bNiR//vx8+umn1K5d2+tYIhIDAr1JT7CsAiqa2cXAz0AboF3GDZxzF598bmYTgf865z4IY0aRkOvcuTNTp06lbdu2JCcnU6xYMa8jiUiMCGvhd86lmlkffKP144DxzrmvzOxu//rkcOYRCTfnHGZG9+7dufbaa+nSpYtm4BORsAp3ix/n3ExgZqZlWRZ851yXcGQSCbXjx48zcOBAihYtyuOPP06jRo1o1KiR17FEJAZpFJFIiH3zzTdceeWVPP/88xw+fBjnAr36VUQk+HLV4jezasA1QElgjHNup5lVAHY5534NRUCRSOWcY/LkyfTu3ZuEhARmzJjBLbfc4nUsEYlxARV+MysIvAb8Dd8leQ74ENgJPA18A2heUZEMfvzxR3r06MGVV17J1KlTKVu2rNeRREQC7up/ArgO6Aicwx+vx58FNA1yLpGI9dNPPwFQrlw5Fi9ezPz581X0RSTPCLTwtwX+6Zx7HdiXad1moFwwQ4lEovT0dEaOHEmFChX44IMPAKhbty5xcXHeBhMRySDQc/wlgQ3ZrMsHFAxOHJHI9Msvv9C5c2dmz55NixYtuOaaa7yOJCKSpUBb/JuBK7NZVwfYFJw4IpFn/vz5JCUlsXDhQkaPHs17773HWWed5XUsEZEsBdrinwwM8k+j+55/mTOzxsB9wNDgRxOJDDt27KBEiRLMmTOHatWqeR1HRCRHgbb4nwY+Aqbw/+f4lwLzgNnOuRdCkE0kz9qyZQvTp08HoEOHDnzxxRcq+iISEQJq8fvvlNfGzEbjG8F/NrAXX9FfFMJ8InnO22+/zV133UVCQgLXX389hQsXpmBBDXMRkciQqwl8nHNLgCUhyiKSpx05coT+/fvzyiuvULduXaZNm0bhwoW9jiUikithn6tfJBIdOXKEOnXq8NVXXzFw4ECGDx9OgQIFvI4lIpJrgc7cl45vtr5sOed0sbJErcKFC9O2bVvq1KnD9ddf73UcEZHTFmiLfxh/LvwlgRvwXcM/MYiZRPKEffv2cffdd9O/f3+uuuoqHnnkEa8jiYicsUAH9w3NarmZxeGbs/9AEDOJeG7p0qW0a9eOnTt30rRpU6666iqvI4mIBMUZ3ZbXP9r/JaB/UNKIeCwtLY3hw4fTsGFD4uPjWbZsGd26dfM6lohI0JxR4fcrCGiaMokKU6dOZciQIbRt25bPP/+c2rVrex1JRCSoAh3cd2EWi+OBy4ARwOpghhIJt71791KyZEnat29PqVKluOmmmzCzU79RRCTCBNri34Jvvv6Mj038//S9vYOeTCQMjh8/Tr9+/bj00kvZsWMHcXFxNGvWTEVfRKJWoKP6u2ax7BjwI7DKf65fJKJs2rSJNm3asHbtWvr166cb64hITDhl4feP3F8LbHfO7Q55IpEQc84xefJkevfuTUJCAh9++CE333yz17FERMIikK5+h+8cfo0QZxEJm48++ojatWuTkpKioi8iMeWULX7nXLqZbQWKhCGPSMisXr2a4sWLU6FCBSZMmEBCQgJxcZpwUkRiS6CD+8YA/c0sPpRhREIhPT2dkSNHctVVV/HAAw8AUKRIERV9EYlJgQ7uSwT+CvxgZrOBHfxxCl/nnHs02OFEztQvv/xC586dmT17Ni1btmTcuHFeRxIR8VS2hd/MfgBaOudSgEEZVt2ZxeYOUOGXPGX9+vVcf/317N+/n5deeom7775bl+mJSMzLqcVfDt+sfDjngjHDn0hY/fWvf6V+/foMHjyYatWqeR1HRCRPUEGXqLJ582batWvHr7/+SqFChXj77bdV9EVEMjhV4c98K16RPOvNN9+kevXqzJw5k/Xr13sdR0QkTzrV4L7HzGxPAPtxzrnOwQgkkluHDx+mf//+jBs3jnr16jFt2jTKlSvndSwRkTzpVIW/OnA8gP2oZ0A807t3byZPnsygQYMYOnQoBQoU8DqSiEiedarC38I5tzIsSURywTnH0aNHKVy4MI899hgdO3bk2muv9TqWiEieF+h1/CJ5xr59++jevTsnTpzgww8/5KKLLuKiiy7yOpaISERQ4Re6TljJwk2Rcf+lpUuX0q5dO3bu3MmIESNwzunafBGRXNDlfBKyot+4cumg7SstLY1hw4bRsGFD4uPjWbZsGQMGDCBfPv0Ki4jkRrYtfk3aE3u2jGjudYRs/e9//yM5OZm2bdvy0ksvUaxYMa8jiYhEJHX1S572ySefUL9+fUqWLMkXX3zBOeec43UkEZGIpla95EnHjh2jX79+NG7cmJdffhlARV9EJAjU4pc8Z9OmTbRp04a1a9fSr18/evTo4XUkEZGoocIvecp7771Hp06dSEhI4MMPP+Tmm2/2OpKISFRRV7/kKeXKlaN+/fqkpKSo6IuIhIAKv3hu1apVDBs2DICaNWsye/ZsypYt63EqEZHopMIvnklPT+ff//43V111Fa+++ir79+/3OpKISNRT4RdP7Nq1i2bNmvHggw9y6623snbtWkqUKOF1LBGRqKfBfRJ2qampNGjQgK1bt/Lyyy/Ts2dPTbsrIhImKvwSNqmpqcTFxZE/f35GjhzJxRdfzGWXXeZ1LBGRmKKufgmLzZs3U79+fV555RUAbrnlFhV9EREPqPBLyL355ptUr16djRs3UrJkSa/jiIjENBV+CZnDhw/TvXt32rRpQ9WqVVm7di2tWrXyOpaISExT4ZeQWb58ORMmTGDQoEEsWrSIcuXKeR1JRCTmaXCfBJVzji+++IKaNWty3XXXsWnTJipUqOB1LBER8VOLX4Jm3759tGrVijp16vDll18CqOiLiOQxavFLUCxZsoR27dqxa9cunn76aapWrep1JBERyYJa/HLGnnjiCRo1akRCQgLLly9nwIAB5MunXy0RkbxI/ztLULRr147PP/+cWrVqeR1FRERyoK5+OS3Tp08nISGBpk2bMmjQIE25KyISIcLe4jezG81sk5l9Z2YPZbG+vZmt8z+WmVlSuDNK9o4dO0bfvn1p0aIFo0aNAlDRFxGJIGEt/GYWB4wGbgKqAG3NrEqmzTYDDZ1z1YDhwNhwZpTsbdy4kXr16vHiiy/Sv39/pk+f7nUkERHJpXB39dcBvnPO/QBgZm8AtwFfn9zAObcsw/YrgPPDmlCy9M0331CrVi0KFy7Mf//7X5o3b+51JBEROQ3h7uovC2zN8Hqbf1l2ugGzQppIcuScA6BixYo88sgjpKSkqOiLiESwcBf+rE4Guyw3NGuMr/APzGZ9DzNbbWard+/eHcSIctLKlSupUaMG3377LWbGoEGDKFOmjNexRETkDIS78G8DLsjw+nxge+aNzKwaMA64zTm3N6sdOefGOudqO+dqly5dOiRhY1V6ejr//ve/ufrqq9m/fz8HDhzwOpKIiARJuAv/KqCimV1sZvFAG2BGxg3M7ELgPaCjc+6bMOeLebt27aJZs2Y8+OCD3Hbbbaxdu5batWt7HUtERIIkrIP7nHOpZtYHmAPEAeOdc1+Z2d3+9cnAEKAk8JL/MrFU55wqT5iMHDmSRYsWkZycTI8ePXSpnohIlLGTg7ciWe3atd3q1au9jhGxLvrHdNIO7WPby105evQomzdvpkqVzFdZiohIXmJma06nYayZ+2Lc5s2b2Tl1IOnHfuX4c+0oVKiQir6ISBTTXP0x7M0336R69er8tm8bxa/pRMGCBb2OJCIiIaYWfww6evQoffv25dVXX+XKK6/kp6Tu5P/LOV7HEhGRMFCLPwYVKFCATZs2MWjQIBYtWqSiLyISQ9TijxHOOcaNG0fLli0pVaoUCxYsoECBAl7HEhGRMFOLPwbs3buXli1b0qNHD5KTkwFU9EVEYpRa/FFu8eLFtG/fnl27dvHss8/Sv39/ryOJiIiHVPij2BtvvEH79u0pX748y5cvp1atWl5HEhERj6mrP4pde+219OnTh88//1xFX0REABX+qDN9+nRuu+02UlNTKV26NM8//zyJiYlexxIRkTxChT9KHDt2jL59+9KiRQu2bt3K3r1Z3tRQRERinAp/FNi4cSP16tXjxRdf5L777mP58uWcc46uzRcRkT/T4L4I55yjbdu2/Pzzz3z00Uc0a9bM60giIpKHqfBHqIMHD1KgQAEKFSrEa6+9RokSJShTpozXsUREJI9TV38EWrlyJdWrV+eBBx4AoGrVqir6IiISEBX+CJKens7TTz/N1VdfTVpaGu3bt/c6koiIRBh19UeIXbt20alTJ+bOnUurVq145ZVXKFGihNexREQkwqjFHyEOHDjAF198wZgxY3j77bdV9EVE5LSoxZ+HnThxgjfffJMOHTpQqVIlNm/eTJEiRbyOJSIiEUwt/jzqhx9+oEGDBnTq1ImlS5cCqOiLiMgZU+HPg6ZNm0b16tX55ptveOedd2jQoIHXkUREJEqo8OcxDzzwAO3atePyyy9n7dq1tGrVyutIIiISRXSOP49p3LgxCQkJDB06lPz59dcjIiLBpcriMecco0eP5vjx49x///00b96c5s2bex1LRESilLr6PbR3715atGhB3759WbJkCc45ryOJiEiUU+H3yKJFi0hKSmLWrFmMGjWK999/HzPzOpaIiEQ5dfV7YPv27dxwww1cdNFFrFixgpo1a3odSUREYoQKfxj9+uuvJCYmUqZMGd555x0aNWpEYmKi17FERCSGqKs/TKZPn87FF1/MrFmzALjllltU9EVEJOxU+EPs2LFj9OnThxYtWlCuXDkqVKjgdSQREYlhKvwhtGHDBurWrcvo0aMZMGAAy5Yto2LFil7HEhGRGKZz/CG0ePFitm/fzkcffUSzZs28jiMiIqIWf7AdOHCAxYsXA9CjRw82btyooi8iInmGCn8QffbZZ9SoUYMWLVrw66+/YmaULFnS61giIiK/U+EPgvT0dJ566inq169Peno6H330kUbsi4hInqRz/GfoxIkT3HzzzXz88ce0bt2asWPHUrx4ca9jiYiIZEmF/wzFx8dTtWpVWrduTffu3TXtroiI5Gkq/KfhxIkTDB48mDZt2lCjRg1GjRrldSQREZGAqPDn0vfff0/btm1ZtWoVxYoVo0aNGl5HEhERCZgKfy68/vrr3H333cTFxfHOO+/QqlUrryOJiIjkikb1B+jdd9+lffv2VKtWjbVr16roi4hIRFKLP5OuE1aycNPu31+7tN+wuAK4tPycdcM9bE1qSsOX1wPrvQspIiJymtTiz+Rk0XfOcXDNh2wf14u0IwewuPwk1miG5YvzOGFoNK5c2usIIiISBmrxZyHt6EFqbnmDGfNm0Lx5cyYOuYFSpUp5HUtEROSMqfBncuynL9nz4TPMPvErzz33HPfee6+uzRcRkaihwp/JwVXvYwUKsnzRXGrWrOl1HBERkaBS4Qe2bt0KwAUXXEDJZv2xuAIq+iIiEpVifnDfBx98QFJSEj169AAgrlAx8sUX8jiViIhIaMRs4T969Ci9e/emZcuWlC9fnhdeeMHrSCIiIiEXk139W7Zs4dZbb+XLL7/k/vvv58knnyQ+Pt7rWCIiIiEXk4W/ZMmSJCYmMnPmTG666Sav44iIiIRNzHT1HzhwgIEDB3L06FESExNZunSpir6IiMScmCj8K1asoHr16owcOZLFixcD6Np8ERGJSVFd+NPT0xkxYgT169fHOceSJUto2rSp17FEREQ8E9WFf8CAATz88MP87W9/Y+3atVx55ZVeRxIREfFUVA7uS09PJ1++fNxzzz1cdtlldOvWTV37IiIieNDiN7MbzWyTmX1nZg9lsd7M7D/+9evMLOAp9E6cOME//vEP2rVrh3OOSpUq0b17dxV9ERERv7AWfjOLA0YDNwFVgLZmViXTZjcBFf2PHsDLgez7u+++4+qrr+aZZ56hRIkSpKamBjG5iIhIdAh3V38d4Dvn3A8AZvYGcBvwdYZtbgMmO+ccsMLMipvZec65HdntdN++fdSsWZO4uDjeeecdWrVqFcqfQUREJGKFu/CXBbZmeL0NqBvANmWBbAv/5i0/UrDMJZS65QHuX5XA/as+ClZeERGRqBLuwp/VyXZ3GttgZj3wnQoAOH7856/X/5x85xnGy7D/p4K2q2hRCtjjdYgYoOMcejrGoadjHB6VT+dN4S7824ALMrw+H9h+GtvgnBsLjAUws9XOudrBjSoZ6RiHh45z6OkYh56OcXiY2erTeV+4R/WvAiqa2cVmFg+0AWZk2mYG0Mk/ur8ecCCn8/siIiISuLC2+J1zqWbWB5gDxAHjnXNfmdnd/vXJwEygGfAdcAToGs6MIiIi0SzsE/g452biK+4ZlyVneO6A3rnc7dggRJOc6RiHh45z6OkYh56OcXic1nE2X50VERGRWBDVc/WLiIjIH0VU4Q/ldL/iE8Axbu8/tuvMbJmZJXmRM5Kd6hhn2O4KM0szs9vDmS9aBHKczayRma01s6/MbFG4M0a6AP6/+IuZfWhmKf5jrDFbuWRm483sFzNbn8363Nc951xEPPANBvweKA/EAylAlUzbNANm4ZsLoB7wmde5I+kR4DG+Cijhf36TjnHwj3GG7RbgGw9zu9e5I+0R4O9ycXyzhl7of32217kj6RHgMR4EPOV/XhrYB8R7nT2SHsA1QE1gfTbrc133IqnF//t0v865E8DJ6X4z+n26X+fcCqC4mZ0X7qAR7JTH2Dm3zDm33/9yBb55FiRwgfweA/QF3gV+CWe4KBLIcW4HvOec+wnAOadjnTuBHGMHJJrvTmlF8RV+3UglF5xzi/Edt+zkuu5FUuHPbirf3G4j2cvt8euG75umBO6Ux9jMygItgWTkdAXyu1wJKGFmn5jZGjPrFLZ00SGQY/wicCm+Sdi+BPo559LDEy9m5Lruhf1yvjMQtOl+JVsBHz8za4yv8NcPaaLoE8gxfg4Y6JxL0y2lT1sgxzk/UAu4FigELDezFc65b0IdLkoEcoybAmuBJsBfgY/NbIlz7mCIs8WSXNe9SCr8QZvuV7IV0PEzs2rAOOAm59zeMGWLFoEc49rAG/6iXwpoZmapzrkPwpIwOgT6/8Ue59xh4LCZLQaSABX+wARyjLsCI5zvZPR3ZrYZuARYGZ6IMSHXdS+Suvo13W/onfIYm9mFwHtAR7WMTsspj7Fz7mLnXDnnXDngHeAeFf1cC+T/i+lAAzPLb2aF8d0pdEOYc0ayQI7xT/h6VDCzc/DdVOaHsKaMfrmuexHT4nea7jfkAjzGQ4CSwEv+Fmmq0804AhbgMZYzFMhxds5tMLPZwDogHRjnnMvykin5swB/l4cDE83sS3xd0gOdc7prXy6Y2TSgEVDKzLYBjwIF4PTrnmbuExERiSGR1NUvIiIiZ0iFX0REJIao8IuIiMQQFX4REZEYosIvIiISQ1T4RbJhZl3MzGXzuC4X+9liZhNDGDXz52XMmWpmP/jv8BXU+yqYWTn/Z3TJsKyLmd2ZxbYnj2W5YGY4Rb5GWRyLn8zsJTMrcZr77G9mfwt2VpFwipjr+EU81Brf7FgZfe1FkFyYCIzB92+8OvAYcLWZVXfOHQ3SZ+wArsR3h7aTuvg/c3ymbT/yb+vFhFr34ptspjC+yWQG4pvp7JbT2Fd/YCm+SaxEIpIKv8iprXXOfed1iFz62X+nLoClZvYrvi8DNxGkouWcO47vDo2BbLsb2B2Mzz0NGzIciwVmdjbQ3czOdc7t9CiTiGfU1S9ymszsBjObaWY7zOyIma03s/vNLO4U7zvXzCaZ2XYzO+5//3/9BenkNoXN7Ckz22xmJ/x/PmJmp/tvdpX/zwr+/Z9nZpPNbI8/wzoz65CbnJm7+s3sE6Ahvp6Fk93rn/jX/aGr33/c1mRxbM7zd8n3z7DsYjObama7/TnWmlnL0zwOAJ/7/7www2dcYWbvmNk2MztqZpvM7EkzK5Rhmy3ARUD7DD/fxAzrk8xshpnt9+/jUzNrcAY5RUJCLX6RU4szs4z/VpxzLg0oD8wHXgCO4bu5zlCgNPBQDvubgq+A/APf7TTPwdcFXRjA/1lzgCr4pjz9EqgHDAbOAu4/jZ/hYv+f/zOzIsAioAQwyJ+hAzDFzAo758YGkjML9wCv4Zu+tad/WXZ3YZsMTDOzKs65jKdN2vn/nAZgZhcAnwG/APfh6zX4O/CumbVwzmWeGz4Q5YA0YEuGZRfiu4vcROBXoCq+6anL45uDHny3Sp4JpOD7e8afBzOrCSwBvgDuwjd16t3APDO7yjn3py85Ip5xzumhhx5ZPPCdr3ZZPJZmsa3h+yL9CLAfyJdh3RZgYobXh4B7c/jcjv7PuSbT8keAE8DZp8jtgCf8eRLwfWnYABwGygB9/Ns0yvS+efgKbFyAOcv599Mlw7JPsjk+J49lOf/rQsAB4F+ZtlsLzMzw+lV8xbVkpu0+xncKJqfj0Mj/mTf4j0Ui0ALfl5Fncnjfyb/LDvjm8C+ZYd0W4LUs3jPff4zjMyyL8y/7wOvfZT30yPhQV7/IqbUErsjw6Aa/d0uPMbMf8RXk34DHgeLA2VnvCvB1u//DzPqZ2eVmlvl+2jcCPwLLzHfnuPz+XoC5+G7OUS+AzIP8eY4Cy/3PmznntgPX4BsD8Emm97yGr7eiSoA5T5vzDTB8F1+3uQGY2eX4bos7OcOmN+JrZR/IdCzmAElmViyAj5uD7+c/CLwPLMbXi/E7MyvmP7XyPXDcv/0UfF8CKua0c//pgIbA20B6hoyG78vUNQFkFAkbFX6RU1vvnFud4bHJf659BnAzvmLfBN+Xgif870nIYX9/97/3QXx3hvvZzIZkOH9/Nr4u9t8yPU7ew7xkAJnH+/PUAEo556o55xb5151F1qPrd2ZYH0jOMzUZ3+j6Rv7XHfF1s0/PsM3ZQCf+fCz+7V8fyLHoje9YXAe8CTTHd9okown4uub/A1zv3763f11Of5fgO15x/n1mztkHKBHEYyZyxnSOX+T0/BXfOf2OzrnXTi40s1NeIuac+wVfUeltZpWBzvgut9sNvAzsBTYDd2Sziy0B5NvhnFudzbp9+O6Lntm5/j/3BpjzTC3Cd7/2Dma2CGgLvOP+eLnhXnznzp/KZh/bA/icb04eCzNbgG+swiAzm+Cc22pmCcBtwFDn3PMn3+TvgQjE//CdEhjNH3srfuecSw9wXyIhp8IvcnpODnD77eQCMysAtM/NTpxzm/AVobuBy/yLZwOtgEPOuY1ByJrZIqC1mV3tnPs0w/J2+M7xbwgwZ1aO4zuXfkrOOWdmU/F9uXgfOJ8/F87Z+K7//8oFYf4B/2f2xzcI7yH/ZxfE12L/LdPmXbLYxXF84xMy7vOwmS3Bd5ricxV5yetU+EVOzwZ85+GfMLM0fEXjvlO9ycz+gu+871Rgo/99t+EbYT/Xv9lUoCsw38xG4htFHo+vl+FWoIVz7sgZZJ8I9APeM7NH8E1O1B5fF3dP51xagDmz8jVwj5n9Hd/EPr/6vzRkZzLwMJCM78qBRZnWD8F3imOxmb2Ir7ejBL4vH+Wdc3+aJfBUnHMpZvYu0M3MnnDObTezFcD9ZrYD2APcCZTN5udrYGY34zs1ssc5twUYgG/swBwzexXfqZRSQE18gyVzuspDJLy8Hl2ohx559cH/j0SvkM366vhmcTuCr3gOA7qTYfS6f7st+Ef142tdjgG+wjdq/iC+QXTtMu07Ad8lYxvxtTL3+bcbCuQ/RW4HPH6Kbc7DN3htj3//64AOGdafMidZj+o/F99gvF/96z7JdCzLZZFllX/dk9lkPR8YB/yMbxDlDnyj+juc4mds5N/vdVmsuxTfJX3PZ/hZZvlz/wK8iG8swB+ufgAuwXfq4Yh/3cRM+3zD//7j/t+JGfgGVXr++6yHHicf5pxDREREYoNGmoqIiMQQFX4REZEYosIvIiISQ1T4RUREYogKv4iISAxR4RcREYkhKvwiIiIxRIVfREQkhqjwi4iIxJD/Aybw71mipckYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc_curve(fpr,tpr,label=None):\n",
    "    plt.plot(fpr,tpr,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('False Positive Rate',fontsize=16)\n",
    "    plt.ylabel('True Positive Rate',fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_roc_curve(fpr,tpr,thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c45491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10f019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9997939e+00, 9.9979389e-01, 9.9901468e-01, 9.9637687e-01,\n",
       "       9.8368025e-01, 9.7946423e-01, 9.4742697e-01, 8.8307303e-01,\n",
       "       3.3811969e-01, 5.7686478e-02, 4.4505805e-02, 1.0505199e-02,\n",
       "       2.1280050e-03, 2.3773313e-04, 8.4111074e-05, 1.8496885e-05,\n",
       "       1.0665892e-05, 1.1101030e-10], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfe03f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd9a853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bba91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "def func(x):\n",
    "    if x>= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y_pred = np.array(list(map(func,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca367596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,) (40,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape,y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02835299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall=recall_score(y_true, y_pred)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f87f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(precision_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c38b6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
